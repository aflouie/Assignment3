---
title: "Homework 3"
author: "Allison Louie"
format:
  html:
    embed-resources: true
---

```{r}
library(tm)
library(dplyr)
library(stringr)
library(tidytext)
library(ggplot2)
pubmed <- read.csv("https://raw.githubusercontent.com/USCbiostats/data-science-data/master/03_pubmed/pubmed.csv")
```

## 1. Tokenize the abstracts and count the number of each token. Do you see anything interesting? Does removing stop words change what tokens appear as the most frequent? What are the 5 most common tokens for each search term after removing stopwords?

```{r}
data_tokens <- pubmed %>%
  unnest_tokens(word, abstract)
token_counts <- data_tokens %>%
  count(word, sort = TRUE)
most_common_tokens <-head(token_counts, 10)
most_common_tokens
```

Most of it has to do with filler words like the, of, and, in, to, a, with, and is. We need to remove these words to see if there are actual content to be observed.

```{r}
data_tokens_filtered <- data_tokens %>%
  filter(!word %in% stopwords("en"))%>%
  filter(!grepl("^[0-9]+$", word))
token_counts_filtered <- data_tokens_filtered %>%
  count(word, sort = TRUE)
most_common_tokens_without_stopwords <- head(token_counts_filtered, 10)
most_common_tokens_without_stopwords
```

These would be the words that are most mentioned while getting rid of any possible numbers that might pop up. Pre would probably be used to be attached to eclampsia, so that might be an issue since there is already an preeclampsia.

## 2. Tokenize the abstracts into bigrams. Find the 10 most common bigrams and visualize them with ggplot2.

```{r}
bigrams <- pubmed %>%
  unnest_tokens(bigram, abstract, token = "ngrams", n = 2)
top_bigrams <- bigrams %>%
  count(bigram, sort = TRUE) %>%
  top_n(10, n)
ggplot(top_bigrams, aes(n, reorder(bigram, n))) +
  geom_col() +
  labs(x = "Frequency", y = "Bigram")
```
A lot of them still have filler words like of the, in the, patients with, and the, to the, and of prostate.  It is not very specific, but we know that these phrases have been utilized a lot.

## 3. Calculate the TF-IDF value for each word-search term combination (here you want the search term to be the "document"). What are the 5 tokens from each search term with the highest TF-IDF value? How are the results different from the answers you got in question 1?

```{r}
pubmed%>%
  unnest_tokens(text, abstract)%>%
  count(text, term)%>%
  bind_tf_idf(text, term, n)%>%
  arrange(desc(tf_idf))%>%
  group_by(term)%>%
  slice(1:5)
```

In this, we see the amount of times certain words appear with certain terms, seeing how it often appears.  Also, it doesn't have stop words in this, being more specific with what appears with each term.  We get a better understanding of what is going on and being associated with each term.
